{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1bonRBSzY1Z"
   },
   "source": [
    "# Crime Patterns in Chicago\n",
    "*Examining the Relationship Between Daytime and Nighttime Crime Rates*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRD9t0ynzga2"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1cRG-I0zov1"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22274,
     "status": "ok",
     "timestamp": 1764381208587,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "DutMkZeHVSpY",
    "outputId": "92a75c8d-6588-4d1e-c6ec-713a9c8d9af0"
   },
   "outputs": [],
   "source": [
    "!pip install astral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8420,
     "status": "ok",
     "timestamp": 1764381217009,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "g8LxCWu8zjwX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "# from google.colab import drive\n",
    "from pyproj import Transformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import display\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9t9iAPIzpwZ"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1764381217623,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "b0f2YZUFzqpK",
    "outputId": "ffa681f7-db86-4f21-8722-645ce658c666"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "# NOTE: To get this working, right click the 'In Data We Trust' folder in\n",
    "#   Google Drive, then add a shortcut. This will then work automatically\n",
    "#   without having to change the directory.\n",
    "proj_dir = '/content/drive/MyDrive/CS326 - In Data We Trust'\n",
    "\n",
    "# This should print the files in the project folder.\n",
    "!ls \"$proj_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2944,
     "status": "ok",
     "timestamp": 1764381220568,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "UuFlg217zv3r",
    "outputId": "806871e5-eaca-4e8e-fdb1-74f132d01e05"
   },
   "outputs": [],
   "source": [
    "# Fix columns names\n",
    "df = pd.read_csv(\"crimes_data_chicago.csv\")\n",
    "df.columns = df.columns.str.replace(r'\\s+', ' ', regex=True)\\\n",
    "                      .str.strip().str.lower()\\\n",
    "                      .str.replace(' ', '_').str.replace('#', '')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1764381220620,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "XwQZ4yB6zyhF",
    "outputId": "18fcdabf-1234-431b-a146-5bc6a1f5969a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvJEBxUIz0Ie"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Av8eh3N2RsX"
   },
   "source": [
    "**NOTE:** PROVIDE REASONING FOR THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1764381220688,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "hfiImWvOz0_s"
   },
   "outputs": [],
   "source": [
    "# Drop additional columns\n",
    "\n",
    "columns_to_drop = ['case', 'x_coordinate', 'y_coordinate', 'location', 'iucr']\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7MmFprMaLF-"
   },
   "source": [
    "Quick analysis of what percentage of rows contain NA location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1764381220819,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "AH240eCw2VQv",
    "outputId": "e82bc93e-8f83-4974-cc94-88f0b943d43c"
   },
   "outputs": [],
   "source": [
    "prev_num_rows = len(df.index)\n",
    "\n",
    "# There are 93 rows where NaN values are in latitude and longitude\n",
    "df = df.dropna(subset=['longitude', 'latitude'])\n",
    "\n",
    "# We only have 1 row that has NaN value that is NOT latitude or longitude\n",
    "# @ index 230265 for Ahmed (Hamood)\n",
    "#   Drop unnecessary columns for modeling/correlation matrix right before preprocessing step\n",
    "#   Save this as a separate dataframe!!!\n",
    "df = df.dropna(subset=['ward'])\n",
    "\n",
    "curr_num_rows = len(df.index)\n",
    "\n",
    "print(curr_num_rows/prev_num_rows*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTGWbXxraHWH"
   },
   "source": [
    "Grouping of less frequent categories under OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1764381221786,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "TcU0pANKzw6v",
    "outputId": "109caf1b-90d5-4549-b7fe-9162bd4104c0"
   },
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "\n",
    "# Display values for location_description, primary_description, secondary_description, fbi_cd\n",
    "categorical_cols_to_aggr = [ \"location_description\", \"primary_description\",  \"secondary_description\", \"fbi_cd\"]\n",
    "for col in categorical_cols_to_aggr:\n",
    "    uniques = df[col].value_counts()\n",
    "    # print(f\"Unique values in {col}: {len(uniques)}\")\n",
    "\n",
    "    counts = df[col].value_counts(normalize=True)\n",
    "    to_keep = counts[counts > threshold].index\n",
    "\n",
    "    df.loc[:, col] = df[col].where(df[col].isin(to_keep), \"OTHER\")\n",
    "\n",
    "    display(df[[col]].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPag27G7aebl"
   },
   "source": [
    "Creating day/time binary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41903,
     "status": "ok",
     "timestamp": 1764381263693,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "lB1EkJsV2xZW"
   },
   "outputs": [],
   "source": [
    "df[\"date_of_occurrence\"] = pd.to_datetime(df[\"date_of_occurrence\"])\n",
    "city = LocationInfo(\"Chicago\", \"USA\", \"America/Chicago\", 41.8781, -87.6298)\n",
    "\n",
    "def is_daytime(ts):\n",
    "  # Check if the timestamp is NaT before localizing\n",
    "  if pd.isna(ts):\n",
    "    return 0 # Or handle missing timestamps as appropriate for your analysis\n",
    "\n",
    "  # Localize the timestamp to the city's timezone, handling ambiguous times by setting them to NaT\n",
    "  ts_localized = ts.tz_localize(city.timezone, ambiguous='NaT')\n",
    "\n",
    "  # Check if localization resulted in NaT (due to ambiguity or original NaT)\n",
    "  if pd.isna(ts_localized):\n",
    "      return 0 # Or handle as appropriate\n",
    "\n",
    "  # Get sunrise and sunset for the date of the localized timestamp\n",
    "  s = sun(city.observer, date=ts_localized.date(), tzinfo=city.timezone)\n",
    "\n",
    "  # Check if sunrise or sunset is NaT\n",
    "  if pd.isna(s[\"sunrise\"]) or pd.isna(s[\"sunset\"]):\n",
    "      return 0 # Or handle as appropriate\n",
    "\n",
    "  return int(s[\"sunrise\"] <= ts_localized <= s[\"sunset\"])\n",
    "\n",
    "# Apply the function to the date_of_occurrence column\n",
    "df[\"is_daytime\"] = df[\"date_of_occurrence\"].apply(is_daytime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1764381263715,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "5rWzafCY0HP6",
    "outputId": "4444748a-ddaa-4a2f-9ca5-d626e0f788ba"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1764381263717,
     "user": {
      "displayName": "Yusuf Ozdemir",
      "userId": "03317550726260970856"
     },
     "user_tz": 300
    },
    "id": "dBf7wzUY8eI0"
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'arrest'] = df['arrest'].map({'Y': True, 'N': False})\n",
    "df.loc[:, 'domestic'] = df['domestic'].map({'Y': True, 'N': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj06hKsHanq7"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MJiPCPVr5zsb"
   },
   "outputs": [],
   "source": [
    "# @title Function for displaying correlation\n",
    "def show_day_night_correlations(df, target_col, loc_col, daytime_col='is_daytime'):\n",
    "    # 1. Ensure target is numeric (0 or 1) for the entire operation\n",
    "    # We create a copy so we don't modify your original dataframe\n",
    "    work_df = df.copy()\n",
    "    work_df[target_col] = work_df[target_col].astype(int)\n",
    "\n",
    "    # 2. TABLE: Calculate correlations split by Day/Night\n",
    "    results = {}\n",
    "    locations = work_df[loc_col].unique()\n",
    "\n",
    "    for loc in locations:\n",
    "        loc_binary = (work_df[loc_col] == loc).astype(int)\n",
    "\n",
    "        # Day Stats\n",
    "        mask_day = work_df[daytime_col] == True\n",
    "        if mask_day.sum() > 0:\n",
    "            r_day, p_day = pearsonr(loc_binary[mask_day], work_df.loc[mask_day, target_col])\n",
    "        else:\n",
    "            r_day, p_day = 0, 1.0\n",
    "\n",
    "        # Night Stats\n",
    "        mask_night = work_df[daytime_col] == False\n",
    "        if mask_night.sum() > 0:\n",
    "            r_night, p_night = pearsonr(loc_binary[mask_night], work_df.loc[mask_night, target_col])\n",
    "        else:\n",
    "            r_night, p_night = 0, 1.0\n",
    "\n",
    "        results[loc] = {\n",
    "            \"Corr_Day\": r_day, \"P_Day\": p_day,\n",
    "            \"Corr_Night\": r_night, \"P_Night\": p_night,\n",
    "            \"Diff (Day-Night)\": r_day - r_night\n",
    "        }\n",
    "\n",
    "    results_df = pd.DataFrame(results).T.sort_values(\"Diff (Day-Night)\", ascending=False)\n",
    "\n",
    "    def style_sig(val):\n",
    "        return 'color: red' if val >= 0.01 else 'color: green'\n",
    "\n",
    "    display(results_df.style.map(style_sig, subset=['P_Day', 'P_Night'])\n",
    "            .format(\"{:.3f}\")\n",
    "            .background_gradient(subset=['Diff (Day-Night)'], cmap='coolwarm'))\n",
    "\n",
    "    # 3. VISUALIZATION: Heatmap\n",
    "    # Now using 'work_df' where target_col is guaranteed to be numeric\n",
    "    pivot_df = work_df.pivot_table(\n",
    "        index=loc_col,\n",
    "        columns=daytime_col,\n",
    "        values=target_col,\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "\n",
    "    pivot_df.columns = [f'Night ({target_col} rate)', f'Day ({target_col} rate)']\n",
    "\n",
    "    plt.figure(figsize=(8, len(pivot_df) * 0.4 + 2))\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"Reds\", fmt=\".1%\", cbar_kws={'label': 'Probability'})\n",
    "    plt.title(f\"Impact of {daytime_col} on {target_col} by Location\")\n",
    "    plt.ylabel(\"Location\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2945,
     "status": "ok",
     "timestamp": 1764341966409,
     "user": {
      "displayName": "Doğaç Eldenk",
      "userId": "17784989456282984533"
     },
     "user_tz": 300
    },
    "id": "turl4GbW747x",
    "outputId": "18ee203c-44f7-4827-a185-db3972e6acdc"
   },
   "outputs": [],
   "source": [
    "show_day_night_correlations(df, 'arrest', 'location_description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gSt1haEJ6Li"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxeMSbLMV6vS"
   },
   "source": [
    "One-hot and feature hash categorical columns,\n",
    "EXCEPT `block` because it has over 28000 unique values and can be represented using `lat` and `lon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = real.copy()\n",
    "df = df.dropna()\n",
    "\n",
    "# Time features \n",
    "df['month'] = df['date_of_occurrence'].dt.month\n",
    "df['weekday'] = df['date_of_occurrence'].dt.weekday\n",
    "df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "\n",
    "df['crime_day_combo'] = df['primary_description'] + \"_\" + df['weekday'].astype(str)\n",
    "df['loc_day_combo'] = df['location_description'] + \"_\" + df['weekday'].astype(str)\n",
    "\n",
    "# GEO CLUSTER FEATURE \n",
    "coords = df[['latitude', 'longitude']].copy()\n",
    "coords = coords.fillna(coords.median())  # safety\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=42, n_init=10)\n",
    "df['geo_cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "\n",
    "# CATEGORICAL ENCODING \n",
    "categorical_cols = [\n",
    "    'location_description',\n",
    "    'beat',\n",
    "    'ward',\n",
    "    'fbi_cd',\n",
    "    'crime_day_combo',\n",
    "    'loc_day_combo',\n",
    "    'geo_cluster',\n",
    "]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "encoded = ohe.fit_transform(df[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "\n",
    "df_encoded = pd.concat([\n",
    "    df.drop(columns=categorical_cols),\n",
    "    encoded_df\n",
    "], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.dropna(axis=0).reset_index(drop=True)\n",
    "\n",
    "y = df_encoded['is_daytime']\n",
    "X = df_encoded.drop(columns=['block', 'is_daytime', 'date_of_occurrence', 'primary_description', 'secondary_description'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model):\n",
    "    cr = cross_validate(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=['f1', 'roc_auc', 'accuracy'],\n",
    "        n_jobs=-1,\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    print(f\"Mean F1 Score: {cr['test_f1'].mean()}\")\n",
    "    print(f\"Mean ROC AUC: {cr['test_roc_auc'].mean()}\")\n",
    "    print(f\"Mean Accuracy: {cr['test_accuracy'].mean()}\")\n",
    "\n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(base_estimator, params):\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_estimator,\n",
    "        param_grid=params,\n",
    "        scoring={\n",
    "            'f1': 'f1',\n",
    "            'roc_auc': 'roc_auc',\n",
    "            'accuracy': 'accuracy'\n",
    "        },\n",
    "        refit='f1',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best params this round:\", grid.best_params_)\n",
    "\n",
    "    best_idx = grid.best_index_\n",
    "    mean_f1 = grid.cv_results_['mean_test_f1'][best_idx]\n",
    "    mean_roc = grid.cv_results_['mean_test_roc_auc'][best_idx]\n",
    "    mean_acc = grid.cv_results_['mean_test_accuracy'][best_idx]\n",
    "\n",
    "    print(f\"Best F1 (CV):       {mean_f1:.4f}\")\n",
    "    print(f\"Best ROC AUC (CV):  {mean_roc:.4f}\")\n",
    "    print(f\"Best Accuracy (CV): {mean_acc:.4f}\")\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = cross_validate_model(KNeighborsClassifier(n_neighbors=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = cross_validate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = cross_validate_model(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = cross_validate_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf_round_1 = {\n",
    "    'n_estimators': [400, 300],\n",
    "}\n",
    "\n",
    "grid1 = grid_search(rf, param_grid_rf_round_1)\n",
    "best_n_estimators = grid1.best_params_['n_estimators']\n",
    "\n",
    "\n",
    "param_grid_rf_round_2 = {\n",
    "    'n_estimators': [best_n_estimators],\n",
    "    'max_depth': [None, 20],\n",
    "    'max_features': ['sqrt', 0.5],\n",
    "}\n",
    "\n",
    "grid2 = grid_search(rf, param_grid_rf_round_2)\n",
    "best_max_depth = grid2.best_params_['max_depth']\n",
    "best_max_features = grid2.best_params_['max_features']\n",
    "\n",
    "\n",
    "param_grid_rf_round_3 = {\n",
    "    'n_estimators': [best_n_estimators],\n",
    "    'max_depth': [best_max_depth],\n",
    "    'max_features': [best_max_features],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 5],\n",
    "}\n",
    "\n",
    "grid3 = grid_search(rf, param_grid_rf_round_3)\n",
    "best_min_split = grid3.best_params_['min_samples_split']\n",
    "best_min_leaf = grid3.best_params_['min_samples_leaf']\n",
    "\n",
    "\n",
    "param_grid_rf_round_4 = {\n",
    "    'n_estimators': [best_n_estimators],\n",
    "    'max_depth': [best_max_depth],\n",
    "    'max_features': [best_max_features],\n",
    "    'min_samples_split': [best_min_split],\n",
    "    'min_samples_leaf': [best_min_leaf],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [None, 0.7],\n",
    "}\n",
    "\n",
    "\n",
    "grid4 = grid_search(rf, param_grid_rf_round_4)\n",
    "best_bootstrap = grid4.best_params_['bootstrap']\n",
    "best_max_samples = grid4.best_params_.get('max_samples', None)\n",
    "\n",
    "\n",
    "param_grid_rf_round_5 = {\n",
    "    'n_estimators': [best_n_estimators],\n",
    "    'max_depth': [best_max_depth],\n",
    "    'max_features': [best_max_features],\n",
    "    'min_samples_split': [best_min_split],\n",
    "    'min_samples_leaf': [best_min_leaf],\n",
    "    'bootstrap': [best_bootstrap],\n",
    "    'max_samples': [best_max_samples],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid5 = grid_search(rf, param_grid_rf_round_5)\n",
    "best_class_weight = grid5.best_params_['class_weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf_final = {\n",
    "                   \n",
    "}\n",
    "\n",
    "final_grid = grid_search(rf, param_grid_rf_final)\n",
    "best_class_weight = grid5.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb = HistGradientBoostingClassifier(\n",
    "    loss='log_loss',\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_cv = cross_validate_model(hgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb = HistGradientBoostingClassifier(\n",
    "    loss='log_loss',\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_hgb_round_1 = {\n",
    "    'learning_rate': [0.03, 0.05, 0.1, 0.2],\n",
    "}\n",
    "grid1 = grid_search(hgb, param_grid_hgb_round_1)\n",
    "best_lr = grid1.best_params_['learning_rate']\n",
    "\n",
    "\n",
    "param_grid_hgb_round_2 = {\n",
    "    'learning_rate': [best_lr],\n",
    "    'max_leaf_nodes': [31, 63, 127, 255],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "}\n",
    "grid2 = grid_search(hgb, param_grid_hgb_round_2)\n",
    "best_leaf_nodes = grid2.best_params_['max_leaf_nodes']\n",
    "best_depth = grid2.best_params_['max_depth']\n",
    "\n",
    "\n",
    "param_grid_hgb_round_3 = {\n",
    "    'learning_rate': [best_lr],\n",
    "    'max_leaf_nodes': [best_leaf_nodes],\n",
    "    'max_depth': [best_depth],\n",
    "    'min_samples_leaf': [10, 20, 50, 100, 200],\n",
    "}\n",
    "grid3 = grid_search(hgb, param_grid_hgb_round_3)\n",
    "best_min_leaf = grid3.best_params_['min_samples_leaf']\n",
    "\n",
    "\n",
    "param_grid_hgb_round_4 = {\n",
    "    'learning_rate': [best_lr],\n",
    "    'max_leaf_nodes': [best_leaf_nodes],\n",
    "    'max_depth': [best_depth],\n",
    "    'min_samples_leaf': [best_min_leaf],\n",
    "    'l2_regularization': [0.0, 0.5, 1.0, 2.0],\n",
    "}\n",
    "grid4 = grid_search(hgb, param_grid_hgb_round_4)\n",
    "best_l2 = grid4.best_params_['l2_regularization']\n",
    "\n",
    "\n",
    "param_grid_hgb_round_5 = {\n",
    "    'learning_rate': [best_lr],\n",
    "    'max_leaf_nodes': [best_leaf_nodes],\n",
    "    'max_depth': [best_depth],\n",
    "    'min_samples_leaf': [best_min_leaf],\n",
    "    'l2_regularization': [best_l2],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "grid5 = grid_search(hgb, param_grid_hgb_round_5)\n",
    "best_class_weight = grid5.best_params_['class_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_hgb_final = {\n",
    "    'learning_rate': [0.02, 0.03, 0.04],        \n",
    "    'max_leaf_nodes': [48, 63, 80],             \n",
    "    'min_samples_leaf': [15, 20, 30],    \n",
    "    'max_depth': [4, 5, 6],                                  \n",
    "    'l2_regularization': [0.5],     \n",
    "    'class_weight': [None],                     \n",
    "}\n",
    "\n",
    "final_grid = grid_search(hgb, param_grid_hgb_final)\n",
    "best_class_weight = grid5.best_params_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
